{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNzgVxVae7SOdE0hG3w9X08",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mridul-sahu/tokenizing-with-sentencepiece/blob/main/Tokenizing_with_SentencePiece.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SentencePiece: Turning Text into Tidy Tokens for Our Translator\n",
        "\n",
        "*(Step One in Building Our English-Spanish Translator: Making Sense of Sentences!)*\n",
        "\n",
        "---\n",
        "\n",
        "## First Things First: Why We Need to 'Tokenize' Our Text (and What That Even Means!)\n",
        "\n",
        "So, you're ready to build an English-to-Spanish translator! That's exciting. But before our fancy Transformer model can start doing its translation magic, we need to get our text data into a shape that computers can actually work with.\n",
        "\n",
        "**The Challenge: Computers Don't \"Read\" Like We Do**\n",
        "\n",
        "Here's a little secret: computers are brilliant at many things, but understanding raw text like \"Hello\" or \"Hola\" isn't one of them, at least not initially. To a computer, a sentence is just a string of characters. To make it learn, we need to convert these sentences into a more structured, numerical format. This conversion process is generally called **tokenization**.\n",
        "\n",
        "**Our Tool for the Job: SentencePiece!**\n",
        "\n",
        "Enter **SentencePiece**. It’s a super handy tool from Google that helps us cleverly break down text into smaller, manageable units called \"tokens\" (often sub-words, like parts of words). Here's why it's a great choice for our project:\n",
        "\n",
        "* **Plays Nicely with (Almost) Any Language:** Whether it's English, Spanish, or something more exotic, SentencePiece is designed to handle it without needing lots of language-specific rules.\n",
        "* **Smart About Unknown Words:** Ever seen a program go \"Huh?\" when it finds a new word? SentencePiece is better than that. It can break down unfamiliar words into smaller pieces it *does* recognize. This is a big plus!\n",
        "* **Works Directly with Raw Text:** We don't need to fuss too much with pre-splitting sentences by spaces. SentencePiece figures out the best way to segment text directly.\n",
        "\n",
        "**Our Goal in This Section:**\n",
        "\n",
        "In this first part of our tutorial, we're going to:\n",
        "1.  Get our English and Spanish sentences ready.\n",
        "2.  Use SentencePiece to \"learn\" the best way to tokenize this combined text.\n",
        "3.  Create our very own custom tokenizer. This tokenizer will be a crucial component that turns any new English or Spanish sentence into a sequence of numerical IDs – the perfect input for our future translation model!\n",
        "\n",
        "Ready to get started with some (lighthearted) tokenizing tomfoolery? Let's go!"
      ],
      "metadata": {
        "id": "dLuEH8tKlc2d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What You'll Need: Assembling Our Tokenizing Toolkit\n",
        "\n",
        "Welcome, Colab adventurer! Before we get SentencePiece to work its magic on our text, let's quickly go over what we'll be using.\n",
        "\n",
        "2.  **The `sentencepiece` Library:** This is our star tokenizing tool.\n",
        "\n",
        "3.  **Our English-Spanish Dataset (`spa-eng.zip`):**\n",
        "    This is the text we'll be teaching SentencePiece to understand. It's a collection of English sentences paired with their Spanish translations. We'll download this dataset directly from the web using its URL:\n",
        "    `http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip`\n",
        "\n",
        "And that's our short list! We're almost ready to get our hands dirty."
      ],
      "metadata": {
        "id": "R0dWIXAtnC20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Summoning Our Tokenizing Sidekick - SentencePiece!\n",
        "\n",
        "Alright, with our toolkit mentally assembled, our first real action is to ensure `sentencepiece` is part of our Python environment. This library is the star of our tokenizing show, the one that will do all the clever text-splitting."
      ],
      "metadata": {
        "id": "f3i1pRrSoI9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "DvZ-yZaTm5B0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Getting Our Text Ready for Tokenizing\n",
        "\n",
        "With SentencePiece installed, our next job is to prepare the actual text it's going to learn from. Think of it as laying out all the different types of word \"blocks\" so SentencePiece can figure out the best way to create its LEGO set.\n",
        "\n",
        "We need to:\n",
        "1.  Get our hands on the English-Spanish sentence pairs.\n",
        "2.  Combine all these sentences (both English and Spanish) into one big file. SentencePiece likes to see all the data it needs to learn from in one place."
      ],
      "metadata": {
        "id": "IVBODnmRpAOV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's get to it!\n",
        "\n",
        "### 2.1. Downloading and Unpacking Our Text Data\n",
        "\n",
        "We'll use the `spa-eng.zip` dataset. We will download this file and extract the `spa.txt` file we need, which contains English sentences and their Spanish translations, tab-separated."
      ],
      "metadata": {
        "id": "Zl2wU2TOpL-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "import pathlib\n",
        "\n",
        "# Setup paths\n",
        "data_dir = pathlib.Path(\"/tmp/data\")\n",
        "zip_file_name = \"spa-eng.zip\"\n",
        "zip_file_path = data_dir / zip_file_name\n",
        "dataset_url = \"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
        "extracted_dir_path = data_dir / \"spa-eng\" # The zip extracts into this subfolder\n",
        "spa_txt_file_path = extracted_dir_path / \"spa.txt\"\n",
        "\n",
        "# Create data directory\n",
        "data_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Download if needed\n",
        "if not zip_file_path.exists():\n",
        "    print(f\"Downloading {zip_file_name}...\")\n",
        "    urllib.request.urlretrieve(dataset_url, zip_file_path)\n",
        "    print(f\"Downloaded to {zip_file_path}\")\n",
        "else:\n",
        "    print(f\"{zip_file_name} already downloaded.\")\n",
        "\n",
        "print(f\"Extracting {zip_file_name}...\")\n",
        "with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(data_dir)\n",
        "print(f\"Extracted. Check for {spa_txt_file_path}\")\n",
        "\n",
        "# Final check\n",
        "if spa_txt_file_path.exists():\n",
        "    print(f\"Great! {spa_txt_file_path} is ready.\")\n",
        "else:\n",
        "    print(f\"Hmm, {spa_txt_file_path} not found. Please check the download/extraction steps.\")"
      ],
      "metadata": {
        "id": "RGtTbWTYouim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Creating a Combined Training File for SentencePiece\n",
        "\n",
        "Alright, we have our `spa.txt` file, where each line typically holds an English sentence and its Spanish translation, separated by a tab. Now, here's a crucial bit: **SentencePiece is smart, but it won't magically know to treat the tab-separated parts as two distinct sentences if we give it that file directly; it learns best when each sentence it needs to process is on its own line.**\n",
        "\n",
        "So, our next task is to create a single, clean text file where *every* sentence (whether it was originally English or Spanish) gets its own dedicated line. This combined file will be the main textbook from which SentencePiece learns its tokenizing craft for both languages.\n",
        "\n",
        "You can use the `sed` command to replace the first tab (`\\t`) on each line with a newline character (`\\n`)."
      ],
      "metadata": {
        "id": "cSkScruBr5MQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sed 's/\\t/\\n/' /tmp/data/spa-eng/spa.txt > /tmp/data/all_english_spanish_for_sp_training.txt"
      ],
      "metadata": {
        "id": "ovJbtmJxpxpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Training Our SentencePiece Model – Let the Learning Begin!\n",
        "\n",
        "Alright, our text data is prepped and waiting in the `all_english_spanish_for_sp_training.txt` file. Now it's time for SentencePiece to actually learn from it. This \"training\" process involves SentencePiece analyzing the text to identify common character sequences and build its vocabulary of subword units.\n",
        "\n",
        "We'll use the `sentencepiece.SentencePieceTrainer.train()` function. It takes a few important settings that tell it how to learn."
      ],
      "metadata": {
        "id": "ZDjia9c1tVO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "import pathlib # If not already imported\n",
        "\n",
        "\n",
        "# --- Parameters for SentencePiece Training ---\n",
        "\n",
        "# Path to the combined text file we created in Step 2\n",
        "input_file = str(data_dir / \"all_english_spanish_for_sp_training.txt\")\n",
        "\n",
        "# Prefix for the output model and vocabulary files.\n",
        "# This will create files like 'eng_spa_spm.model' and 'eng_spa_spm.vocab'\n",
        "# in your 'data_dir'.\n",
        "model_prefix = str(data_dir / 'eng_spa_spm')\n",
        "\n",
        "# Desired vocabulary size. This is how many unique subword \"pieces\" our model will know.\n",
        "# 16000 is a common starting point, but you can experiment.\n",
        "vocab_size = 16000\n",
        "\n",
        "# Model type: 'unigram' (default) or 'bpe'.\n",
        "# Both are good subword algorithms. 'unigram' often works very well.\n",
        "model_type = 'unigram'\n",
        "\n",
        "# Character coverage: For languages like English/Spanish using Latin scripts,\n",
        "# 0.9995 is a good default. This ensures most characters are included.\n",
        "character_coverage = 0.9995\n",
        "\n",
        "# Special Token IDs: It's wise to define these explicitly for consistency.\n",
        "# We'll use these IDs later when preparing data for our Transformer.\n",
        "#   ID 0: <pad> (Padding token)\n",
        "#   ID 1: <s>   (Beginning Of Sentence - BOS)\n",
        "#   ID 2: </s>  (End Of Sentence - EOS)\n",
        "#   ID 3: <unk> (Unknown token)\n",
        "pad_id = 0\n",
        "bos_id = 1\n",
        "eos_id = 2\n",
        "unk_id = 3\n",
        "\n",
        "# Let's train this thing!\n",
        "try:\n",
        "  print(f\"Starting SentencePiece model training with input: {input_file}\")\n",
        "  spm.SentencePieceTrainer.train(\n",
        "      input=input_file,\n",
        "      model_prefix=model_prefix,\n",
        "      vocab_size=vocab_size,\n",
        "      model_type=model_type,\n",
        "      character_coverage=character_coverage,\n",
        "      pad_id=pad_id,\n",
        "      bos_id=bos_id,\n",
        "      eos_id=eos_id,\n",
        "      unk_id=unk_id,\n",
        "      # By default, SentencePiece normalizes text (e.g., NFKC).\n",
        "      # You can add more options if needed, e.g.:\n",
        "      # normalization_rule_name='nmt_nfkc_cf'\n",
        "  )\n",
        "  print(f\"SentencePiece model training complete!\")\n",
        "  print(f\"Model saved as: {model_prefix}.model\")\n",
        "  print(f\"Vocabulary saved as: {model_prefix}.vocab\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: Input file not found at {input_file}. Please ensure Step 2 ran correctly and the file exists.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during SentencePiece training: {e}\")"
      ],
      "metadata": {
        "id": "yh4D5IbTtgSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Taking Our New Tokenizer for a Spin! (Loading & Testing)\n",
        "\n",
        "Fantastic! We've successfully trained our SentencePiece model. It's crunched through all those English and Spanish sentences and has learned a vocabulary of subword \"pieces.\" Now, let's load this trained model and see it in action.\n",
        "\n",
        "We'll use the `.model` file that was generated in the previous step."
      ],
      "metadata": {
        "id": "xvR7F1nvwq-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Define paths (ensure these match what you used in Step 3) ---\n",
        "model_prefix_name = 'eng_spa_spm' # This was the base name for our model files\n",
        "model_file_path = str(data_dir / f\"{model_prefix_name}.model\")\n",
        "\n",
        "# --- Load the trained SentencePiece model ---\n",
        "try:\n",
        "    spp = spm.SentencePieceProcessor(model_file=model_file_path)\n",
        "    print(f\"Successfully loaded SentencePiece model from: {model_file_path}\")\n",
        "\n",
        "    # --- Let's test it with some sample sentences! ---\n",
        "    sample_eng = \"This is a new sentence to test our tokenizer.\"\n",
        "    sample_spa = \"Esta es una nueva frase para probar nuestro tokenizador.\"\n",
        "\n",
        "    print(f\"\\n--- Testing English: \\\"{sample_eng}\\\" ---\")\n",
        "    # 1. Encode text into a sequence of subword strings (pieces)\n",
        "    eng_pieces = spp.encode_as_pieces(sample_eng)\n",
        "    print(f\"Tokenized into pieces: {eng_pieces}\")\n",
        "\n",
        "    # 2. Encode text into a sequence of integer IDs\n",
        "    eng_ids = spp.encode_as_ids(sample_eng)\n",
        "    print(f\"Encoded into IDs: {eng_ids}\")\n",
        "\n",
        "    # 3. Decode IDs back to text\n",
        "    decoded_eng_from_ids = spp.decode_ids(eng_ids)\n",
        "    print(f\"Decoded from IDs: \\\"{decoded_eng_from_ids}\\\"\")\n",
        "    assert decoded_eng_from_ids == sample_eng # Check if reversible\n",
        "\n",
        "    print(f\"\\n--- Testing Spanish: \\\"{sample_spa}\\\" ---\")\n",
        "    # 1. Encode text into pieces\n",
        "    spa_pieces = spp.encode_as_pieces(sample_spa)\n",
        "    print(f\"Tokenized into pieces: {spa_pieces}\")\n",
        "\n",
        "    # 2. Encode text into IDs\n",
        "    spa_ids = spp.encode_as_ids(sample_spa)\n",
        "    print(f\"Encoded into IDs: {spa_ids}\")\n",
        "\n",
        "    # 3. Decode IDs back to text\n",
        "    decoded_spa_from_ids = spp.decode_ids(spa_ids)\n",
        "    print(f\"Decoded from IDs: \\\"{decoded_spa_from_ids}\\\"\")\n",
        "    assert decoded_spa_from_ids == sample_spa # Check if reversible\n",
        "\n",
        "    # --- Check vocabulary size and special token IDs ---\n",
        "    print(f\"\\n--- Tokenizer Properties ---\")\n",
        "    print(f\"Vocabulary size (spp.get_piece_size()): {spp.get_piece_size()}\")\n",
        "    print(f\"PAD ID (spp.pad_id()): {spp.pad_id()}\")\n",
        "    print(f\"BOS ID (spp.bos_id()): {spp.bos_id()}\")\n",
        "    print(f\"EOS ID (spp.eos_id()): {spp.eos_id()}\")\n",
        "    print(f\"UNK ID (spp.unk_id()): {spp.unk_id()}\")\n",
        "\n",
        "    # Example: How you'd prepare a sequence for a decoder (e.g., Spanish target)\n",
        "    # by adding BOS (Beginning Of Sentence) and EOS (End Of Sentence) tokens.\n",
        "    spa_ids_for_decoder = [spp.bos_id()] + spa_ids + [spp.eos_id()]\n",
        "    print(f\"\\nSpanish IDs with BOS/EOS for decoder: {spa_ids_for_decoder}\")\n",
        "    print(f\"Decoded version with BOS/EOS: \\\"{spp.decode_ids(spa_ids_for_decoder)}\\\"\")\n",
        "    # Note: The decoded string will include the string representations of BOS (<s>) and EOS (</s>)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: Model file not found at {model_file_path}. Please ensure Step 3 ran correctly and the model file exists.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "sxpDwZQ0v-tR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer Mission Accomplished! What We've Done (and What's Next)\n",
        "\n",
        "And just like that, you've successfully navigated the world of SentencePiece and emerged with a custom-trained tokenizer for our English-Spanish translation project! Give yourself a pat on the back – that was some solid work.\n",
        "\n",
        "**Let's quickly recap what we've achieved in this part:**\n",
        "\n",
        "1.  **Set up our tools:** We got SentencePiece installed.\n",
        "2.  **Prepared our linguistic ingredients:** We took the `spa-eng.txt` dataset, processed it, and created a clean training file (`all_english_spanish_for_sp_training.txt`) with all our English and Spanish sentences ready for SentencePiece.\n",
        "3.  **Trained the tokenizer:** We taught SentencePiece to identify common subword units from our combined English-Spanish text, resulting in our very own `.model` and `.vocab` files.\n",
        "4.  **Took it for a test drive:** We loaded our trained tokenizer and confirmed it can convert text into numerical IDs (and pieces) and, just as importantly, turn those IDs back into readable text. We also verified our special token IDs (`<pad>`, `<s>`, `</s>`, `<unk>`).\n",
        "\n",
        "**Why was this so important?**\n",
        "\n",
        "This tokenizer is a *critical* first step. It acts as the bridge between the human-readable language in our dataset and the numerical format that our neural network (the Transformer model we'll build later) can understand and learn from. Without good tokenization, our translation model wouldn't even get off the starting block!\n",
        "\n",
        "**What's Next on Our Adventure?**\n",
        "\n",
        "Now that we have this powerful tool to turn words into tidy, numerical tokens, we're all set for the next exciting phase: actually building and training our English-to-Spanish Transformer model using Flax NNX!\n",
        "\n",
        "We'll use the `eng_spa_spm.model` file we just created to process our entire dataset into a format suitable for training our neural network. So keep that model file safe!\n",
        "\n",
        "Thanks for following along with this tokenizing journey. Get ready for some deep learning action in the next part!"
      ],
      "metadata": {
        "id": "DKJqXbCx-Yei"
      }
    }
  ]
}